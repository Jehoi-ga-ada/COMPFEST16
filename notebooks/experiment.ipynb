{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 19:25:49.524295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-22 19:25:49.620245: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-22 19:25:49.648001: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-22 19:25:49.801980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-22 19:25:51.505546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artificial Intelligence/Machine Learning (AI/ML) Engineer</th>\n",
       "      <th>Business Analyst</th>\n",
       "      <th>Cloud Engineer</th>\n",
       "      <th>Cybersecurity Analyst</th>\n",
       "      <th>Data Engineer</th>\n",
       "      <th>Data Scientist</th>\n",
       "      <th>Database Administrator (DBA)</th>\n",
       "      <th>DevOps Engineer</th>\n",
       "      <th>IT Consultant</th>\n",
       "      <th>IT Manager</th>\n",
       "      <th>IT Project Manager</th>\n",
       "      <th>IT Support Specialist</th>\n",
       "      <th>Information Security Manager</th>\n",
       "      <th>Network Administrator</th>\n",
       "      <th>Quality Assurance (QA) Tester/Engineer</th>\n",
       "      <th>Software Developer/Engineer</th>\n",
       "      <th>Systems Analyst</th>\n",
       "      <th>Systems Architect</th>\n",
       "      <th>Technical Support Engineer</th>\n",
       "      <th>Web Developer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-12-12</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>6.533333</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>9.966667</td>\n",
       "      <td>12.866667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>4.566667</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>3.766667</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.966667</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>17.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-13</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>6.966667</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>10.466667</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>5.366667</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>2.766667</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>18.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-14</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5.233333</td>\n",
       "      <td>3.233333</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>13.433333</td>\n",
       "      <td>5.466667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>4.033333</td>\n",
       "      <td>22.433333</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>9.566667</td>\n",
       "      <td>18.133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Artificial Intelligence/Machine Learning (AI/ML) Engineer  \\\n",
       "2021-12-12                                           0.266667           \n",
       "2021-12-13                                           0.266667           \n",
       "2021-12-14                                           0.266667           \n",
       "\n",
       "            Business Analyst  Cloud Engineer  Cybersecurity Analyst  \\\n",
       "2021-12-12          6.533333        1.200000                    1.1   \n",
       "2021-12-13          6.966667        1.266667                    1.1   \n",
       "2021-12-14          7.166667        1.366667                    1.1   \n",
       "\n",
       "            Data Engineer  Data Scientist  Database Administrator (DBA)  \\\n",
       "2021-12-12       5.133333        2.900000                           1.3   \n",
       "2021-12-13       5.200000        3.066667                           1.3   \n",
       "2021-12-14       5.233333        3.233333                           1.3   \n",
       "\n",
       "            DevOps Engineer  IT Consultant  IT Manager  IT Project Manager  \\\n",
       "2021-12-12         3.033333       9.966667   12.866667            5.200000   \n",
       "2021-12-13         3.200000      10.466667   13.100000            5.366667   \n",
       "2021-12-14         3.466667      10.600000   13.433333            5.466667   \n",
       "\n",
       "            IT Support Specialist  Information Security Manager  \\\n",
       "2021-12-12               4.566667                      2.500000   \n",
       "2021-12-13               4.766667                      2.566667   \n",
       "2021-12-14               4.900000                      2.666667   \n",
       "\n",
       "            Network Administrator  Quality Assurance (QA) Tester/Engineer  \\\n",
       "2021-12-12               2.733333                                3.766667   \n",
       "2021-12-13               2.766667                                3.900000   \n",
       "2021-12-14               2.833333                                4.033333   \n",
       "\n",
       "            Software Developer/Engineer  Systems Analyst  Systems Architect  \\\n",
       "2021-12-12                    20.833333              2.7           2.966667   \n",
       "2021-12-13                    21.666667              2.8           3.000000   \n",
       "2021-12-14                    22.433333              2.9           3.066667   \n",
       "\n",
       "            Technical Support Engineer  Web Developer  \n",
       "2021-12-12                    8.800000      17.866667  \n",
       "2021-12-13                    9.200000      18.133333  \n",
       "2021-12-14                    9.566667      18.133333  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"../backend/src/data/\"\n",
    "df = pd.read_csv(DATA_PATH+'job_demand_30.csv', index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll experiment using the Software Engineer column first and then we try the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiments\n",
    "\n",
    "1. Vanilla-LSTM\n",
    "2. N-BEATS\n",
    "3. Ensemble Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_scaled_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implement MASE (assuming no seasonality of data).\n",
    "    \"\"\"\n",
    "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "    # Find MAE of naive forecast (no seasonality)\n",
    "    mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)\n",
    "\n",
    "    return mae / mae_naive_no_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_pred):\n",
    "    # Make sure float32 (for metric calculations)\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    # Calculate various metrics\n",
    "    mae = keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "    \n",
    "    return {\"mae\": tf.reduce_mean(mae.numpy()),\n",
    "            \"mse\": tf.reduce_mean(mse.numpy()),\n",
    "            \"rmse\": tf.reduce_mean(rmse.numpy()),\n",
    "            \"mape\": tf.reduce_mean(mape.numpy()),\n",
    "            \"mase\": tf.reduce_mean(mase.numpy())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_demand = df['Software Developer/Engineer'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelled_windows(x, horizon=1):\n",
    "  return x[:, :-horizon], x[:, -horizon:]\n",
    "\n",
    "def make_windows(x, window_size=7, horizon=1):\n",
    "  \"\"\"\n",
    "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
    "  \"\"\"\n",
    "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
    "\n",
    "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T\n",
    "\n",
    "  windowed_array = x[window_indexes]\n",
    "\n",
    "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "\n",
    "  return windows, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 60\n",
    "HORIZON = 30\n",
    "full_windows, full_labels = make_windows(se_demand, window_size=60, horizon=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_splits(windows, labels, test_split=0.2):\n",
    "    \"\"\"\n",
    "    Splits matching pairs of windows and labels into train and test splits.\n",
    "    \"\"\"\n",
    "    split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test\n",
    "    train_windows = windows[:split_size]\n",
    "    train_labels = labels[:split_size]\n",
    "    test_windows = windows[split_size:]\n",
    "    test_labels = labels[split_size:]\n",
    "    return train_windows, test_windows, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 35, 138, 35)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "    return keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n",
    "                                                verbose=0, # only output a limited amount of text\n",
    "                                                save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x73ee2212d5e0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "def transform(x):\n",
    "    import tensorflow as tf\n",
    "    return tf.expand_dims(x, axis=1)\n",
    "\n",
    "inputs = layers.Input(shape=[WINDOW_SIZE])\n",
    "x = layers.Lambda(transform)(inputs)\n",
    "x = layers.LSTM(128, activation='relu')(x)\n",
    "output = layers.Dense(HORIZON)(x)\n",
    "model_0 = keras.Model(inputs=inputs, outputs=output, name='model_0_lstm')\n",
    "\n",
    "model_0.compile(loss='mae',\n",
    "                optimizer=keras.optimizers.Adam())\n",
    "\n",
    "model_0.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            verbose=0,\n",
    "            batch_size=32,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_0.name+'.keras')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.2903  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.429537773132324"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0 = keras.models.load_model(\"model_experiments/model_0_lstm.keras\",\n",
    "                                  custom_objects={'transform': transform},\n",
    "                                  safe_mode=False)\n",
    "model_0.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 30), dtype=float32, numpy=\n",
       "array([[ 98.58131 ,  97.89175 ,  98.70699 ,  99.11427 , 101.08131 ,\n",
       "         98.70045 ,  99.369354, 102.08051 , 101.547775, 101.116066,\n",
       "        102.921684, 106.82257 , 102.593704, 105.36856 , 104.79472 ,\n",
       "        103.59391 , 100.82244 , 102.893936, 104.83956 ,  99.28092 ,\n",
       "         97.217026, 100.783745, 100.89875 ,  99.09374 ,  98.41969 ,\n",
       "        100.30977 ,  99.7244  ,  97.795494,  99.57678 ,  98.54738 ],\n",
       "       [ 98.17683 ,  97.03095 ,  98.040306,  98.66959 , 100.158066,\n",
       "         97.79937 ,  99.090576, 102.199005, 100.79794 , 100.799904,\n",
       "        102.8551  , 106.11826 , 101.67517 , 105.361046, 104.10607 ,\n",
       "        102.82795 , 100.420425, 102.381836, 104.22042 ,  99.25396 ,\n",
       "         96.95264 , 100.56793 , 100.66818 ,  98.22179 ,  98.658066,\n",
       "         99.91066 ,  99.213646,  97.4239  ,  98.60182 ,  98.28718 ],\n",
       "       [ 97.98418 ,  96.21365 ,  97.327385,  98.043015,  98.97754 ,\n",
       "         96.81642 ,  98.9241  , 102.24777 , 100.12222 , 100.283936,\n",
       "        102.34069 , 105.03171 , 100.41914 , 104.8271  , 103.31449 ,\n",
       "        101.89553 ,  99.615425, 101.26081 , 103.41587 ,  99.0407  ,\n",
       "         96.46965 , 100.16979 , 100.56067 ,  97.195786,  98.50326 ,\n",
       "         99.168816,  98.54474 ,  97.20239 ,  97.559364,  98.08368 ],\n",
       "       [ 97.26157 ,  95.427635,  96.12155 ,  97.14992 ,  98.23739 ,\n",
       "         95.54718 ,  98.17357 , 101.76918 ,  98.795044,  99.04511 ,\n",
       "        101.81517 , 104.089066,  99.23483 , 103.94828 , 102.30397 ,\n",
       "        100.30801 ,  98.8789  , 100.57296 , 102.47488 ,  98.42867 ,\n",
       "         96.153275,  99.528336,  99.89522 ,  96.268585,  98.03172 ,\n",
       "         98.03655 ,  97.77361 ,  96.50375 ,  96.77769 ,  97.466354],\n",
       "       [ 96.52637 ,  94.83119 ,  95.78794 ,  96.80524 ,  97.49486 ,\n",
       "         94.94429 ,  97.94739 , 100.88352 ,  98.01295 ,  98.58717 ,\n",
       "        100.99044 , 102.65446 ,  98.306435, 102.53495 , 101.047356,\n",
       "         99.01236 ,  98.15911 ,  99.820305, 100.856346,  97.867   ,\n",
       "         96.271904,  98.75707 ,  99.6733  ,  96.229385,  98.004395,\n",
       "         97.35157 ,  96.903336,  96.50823 ,  96.32314 ,  97.28398 ],\n",
       "       [ 95.591156,  93.49646 ,  94.97245 ,  96.33175 ,  96.95737 ,\n",
       "         93.77277 ,  96.78245 ,  99.940025,  96.64253 ,  97.839005,\n",
       "        100.30434 , 101.836494,  97.15975 , 101.50676 , 100.01964 ,\n",
       "         97.19285 ,  97.420334,  99.531815,  99.62075 ,  97.69165 ,\n",
       "         96.090126,  98.15354 ,  99.196556,  95.77831 ,  97.82711 ,\n",
       "         96.51275 ,  96.163864,  95.95032 ,  95.86329 ,  96.93655 ],\n",
       "       [ 94.40374 ,  92.487144,  94.24503 ,  95.66733 ,  96.013275,\n",
       "         92.4419  ,  95.97965 ,  99.35463 ,  95.797325,  97.168655,\n",
       "         99.79725 , 100.81578 ,  96.49377 , 100.552475,  98.90266 ,\n",
       "         96.02176 ,  96.953476,  98.91858 ,  98.203156,  97.18967 ,\n",
       "         96.3139  ,  97.54839 ,  98.71664 ,  95.415764,  98.16553 ,\n",
       "         96.28536 ,  95.39233 ,  95.749626,  95.14091 ,  96.428375],\n",
       "       [ 93.73691 ,  92.19093 ,  93.59644 ,  95.175674,  95.46246 ,\n",
       "         91.63799 ,  95.69552 ,  98.88098 ,  95.2848  ,  96.81252 ,\n",
       "         99.286835,  99.913124,  96.04449 ,  99.69291 ,  97.873924,\n",
       "         95.35985 ,  96.587395,  98.43624 ,  97.209206,  96.59662 ,\n",
       "         96.96191 ,  96.816345,  98.465645,  95.03266 ,  98.45858 ,\n",
       "         96.10086 ,  94.8366  ,  95.59054 ,  94.473   ,  95.73797 ],\n",
       "       [ 93.413445,  92.17581 ,  93.24895 ,  94.79503 ,  95.14479 ,\n",
       "         91.195786,  95.476204,  98.19303 ,  94.78432 ,  96.348015,\n",
       "         98.73681 ,  98.85284 ,  95.55647 ,  99.14402 ,  97.5002  ,\n",
       "         94.86171 ,  96.36867 ,  98.27634 ,  96.4989  ,  96.530075,\n",
       "         97.217186,  96.244225,  98.178955,  94.97274 ,  98.32741 ,\n",
       "         95.91139 ,  94.489685,  95.52094 ,  93.83758 ,  95.46941 ],\n",
       "       [ 93.2293  ,  92.13397 ,  92.82121 ,  94.21182 ,  94.8634  ,\n",
       "         90.87895 ,  95.29741 ,  97.691345,  94.38197 ,  95.92077 ,\n",
       "         98.15858 ,  98.1088  ,  95.01093 ,  98.466805,  97.07455 ,\n",
       "         94.45026 ,  95.68054 ,  97.7868  ,  95.76481 ,  96.26093 ,\n",
       "         97.21146 ,  95.45016 ,  97.65558 ,  94.61307 ,  97.61096 ,\n",
       "         95.43997 ,  94.14848 ,  95.381485,  93.34768 ,  95.07901 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with our LSTM model\n",
    "model_0_preds = model_0(test_windows)\n",
    "model_0_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': <tf.Tensor: shape=(), dtype=float32, numpy=11.429537>,\n",
       " 'mse': <tf.Tensor: shape=(), dtype=float32, numpy=159.11499>,\n",
       " 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=12.430287>,\n",
       " 'mape': <tf.Tensor: shape=(), dtype=float32, numpy=14.108284>,\n",
       " 'mase': <tf.Tensor: shape=(), dtype=float32, numpy=12.563094>}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_0_preds)\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: N-BEATS Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBeatsBlock(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                input_size: int,\n",
    "                theta_size: int,\n",
    "                horizon: int,\n",
    "                n_neurons: int,\n",
    "                n_layers: int,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_size = input_size\n",
    "        self.theta_size = theta_size\n",
    "        self.horizon = horizon\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Block contains stack of 4 fully connected layers each has ReLU activation\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n",
    "        # Output of block is a theta layer with linear activation\n",
    "        self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")\n",
    "\n",
    "    def call(self, inputs): # the call method is what runs when the layer is called \n",
    "        x = inputs \n",
    "        for layer in self.hidden: # pass inputs through each hidden layer \n",
    "            x = layer(x)\n",
    "        theta = self.theta_layer(x) \n",
    "        # Output the backcast and forecast from theta\n",
    "        backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 60), dtype=tf.float64, name=None), TensorSpec(shape=(None, 30), dtype=tf.float64, name=None))>,\n",
       " <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 60), dtype=tf.float64, name=None), TensorSpec(shape=(None, 30), dtype=tf.float64, name=None))>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Turn tensor slices to dataset\n",
    "train_features_dataset = tf.data.Dataset.from_tensor_slices(train_windows)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "\n",
    "test_features_dataset = tf.data.Dataset.from_tensor_slices(test_windows)\n",
    "test_labels_dataset = tf.data.Dataset.from_tensor_slices(test_labels)\n",
    "\n",
    "# 2. Combine features and labels\n",
    "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
    "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
    "\n",
    "# 3. Batch and prefetch for optimal performance\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1830)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 5000 \n",
    "N_NEURONS = 512 \n",
    "N_LAYERS = 4\n",
    "N_STACKS = 30\n",
    "\n",
    "INPUT_SIZE = WINDOW_SIZE * HORIZON \n",
    "THETA_SIZE = INPUT_SIZE + HORIZON\n",
    "\n",
    "INPUT_SIZE, THETA_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 140.3158 - mae: 140.3158 - mse: 38002.9531 - val_loss: 14.8837 - val_mae: 14.8837 - val_mse: 367.2444 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 595ms/step - loss: 86.5311 - mae: 86.5311 - mse: 19570.8281 - val_loss: 23.2300 - val_mae: 23.2300 - val_mse: 756.3934 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 592ms/step - loss: 29.2403 - mae: 29.2403 - mse: 1187.4751 - val_loss: 10.9405 - val_mae: 10.9405 - val_mse: 178.1383 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 585ms/step - loss: 19.2867 - mae: 19.2867 - mse: 484.6366 - val_loss: 9.6520 - val_mae: 9.6520 - val_mse: 172.3224 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 586ms/step - loss: 16.9813 - mae: 16.9813 - mse: 388.1904 - val_loss: 15.4309 - val_mae: 15.4309 - val_mse: 298.3350 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 575ms/step - loss: 11.4887 - mae: 11.4887 - mse: 195.5195 - val_loss: 9.3237 - val_mae: 9.3237 - val_mse: 148.3843 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 588ms/step - loss: 13.1752 - mae: 13.1752 - mse: 231.2686 - val_loss: 8.3227 - val_mae: 8.3227 - val_mse: 119.6172 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528ms/step - loss: 10.5197 - mae: 10.5197 - mse: 151.1244 - val_loss: 12.3316 - val_mae: 12.3316 - val_mse: 209.5701 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535ms/step - loss: 6.1339 - mae: 6.1339 - mse: 57.9255 - val_loss: 10.5023 - val_mae: 10.5023 - val_mse: 195.4550 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 580ms/step - loss: 9.8229 - mae: 9.8229 - mse: 135.1398 - val_loss: 7.3191 - val_mae: 7.3191 - val_mse: 93.3546 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 626ms/step - loss: 7.9000 - mae: 7.9000 - mse: 101.5869 - val_loss: 16.7402 - val_mae: 16.7402 - val_mse: 394.9446 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 583ms/step - loss: 9.5872 - mae: 9.5872 - mse: 137.0559 - val_loss: 7.1465 - val_mae: 7.1465 - val_mse: 86.0334 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 527ms/step - loss: 12.9060 - mae: 12.9060 - mse: 227.2383 - val_loss: 12.6259 - val_mae: 12.6259 - val_mse: 216.8175 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519ms/step - loss: 7.0056 - mae: 7.0056 - mse: 78.5637 - val_loss: 7.5208 - val_mae: 7.5208 - val_mse: 100.4776 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524ms/step - loss: 10.6912 - mae: 10.6912 - mse: 155.5876 - val_loss: 10.7735 - val_mae: 10.7735 - val_mse: 170.7442 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535ms/step - loss: 5.5965 - mae: 5.5965 - mse: 52.5736 - val_loss: 8.4929 - val_mae: 8.4929 - val_mse: 147.3141 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 568ms/step - loss: 11.0146 - mae: 11.0146 - mse: 161.8249 - val_loss: 9.3589 - val_mae: 9.3589 - val_mse: 126.3003 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 522ms/step - loss: 6.0609 - mae: 6.0609 - mse: 59.7206 - val_loss: 10.0778 - val_mae: 10.0778 - val_mse: 189.6249 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 516ms/step - loss: 10.2233 - mae: 10.2233 - mse: 141.1535 - val_loss: 9.4621 - val_mae: 9.4621 - val_mse: 146.9716 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519ms/step - loss: 6.4256 - mae: 6.4256 - mse: 68.1151 - val_loss: 9.3398 - val_mae: 9.3398 - val_mse: 135.3501 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 573ms/step - loss: 8.9359 - mae: 8.9359 - mse: 117.0439 - val_loss: 13.1400 - val_mae: 13.1400 - val_mse: 239.4738 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 586ms/step - loss: 6.0061 - mae: 6.0061 - mse: 54.6703 - val_loss: 8.4722 - val_mae: 8.4722 - val_mse: 156.9402 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 508ms/step - loss: 14.4250 - mae: 14.4250 - mse: 268.6906 - val_loss: 7.8669 - val_mae: 7.8669 - val_mse: 108.6943 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 510ms/step - loss: 11.9833 - mae: 11.9833 - mse: 199.0458 - val_loss: 14.6719 - val_mae: 14.6719 - val_mse: 283.8152 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 581ms/step - loss: 6.5234 - mae: 6.5234 - mse: 69.8568 - val_loss: 6.7745 - val_mae: 6.7745 - val_mse: 89.6115 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 594ms/step - loss: 8.5699 - mae: 8.5699 - mse: 105.3899 - val_loss: 11.6253 - val_mae: 11.6253 - val_mse: 228.6280 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 592ms/step - loss: 5.0471 - mae: 5.0471 - mse: 38.2684 - val_loss: 6.0472 - val_mae: 6.0472 - val_mse: 73.7039 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 483ms/step - loss: 6.2516 - mae: 6.2516 - mse: 57.0223 - val_loss: 7.2287 - val_mae: 7.2287 - val_mse: 88.1161 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 514ms/step - loss: 4.4660 - mae: 4.4660 - mse: 32.8491 - val_loss: 8.6865 - val_mae: 8.6865 - val_mse: 153.3279 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 501ms/step - loss: 7.3537 - mae: 7.3537 - mse: 76.1441 - val_loss: 8.2443 - val_mae: 8.2443 - val_mse: 117.9115 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 506ms/step - loss: 5.0995 - mae: 5.0995 - mse: 39.8605 - val_loss: 12.7252 - val_mae: 12.7252 - val_mse: 316.4097 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 525ms/step - loss: 15.8779 - mae: 15.8779 - mse: 316.3417 - val_loss: 9.2152 - val_mae: 9.2152 - val_mse: 138.4731 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 548ms/step - loss: 10.3528 - mae: 10.3528 - mse: 159.2659 - val_loss: 12.1540 - val_mae: 12.1540 - val_mse: 215.7712 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 538ms/step - loss: 7.5169 - mae: 7.5169 - mse: 91.3930 - val_loss: 8.3335 - val_mae: 8.3335 - val_mse: 109.8537 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535ms/step - loss: 9.9032 - mae: 9.9032 - mse: 135.8320 - val_loss: 10.9500 - val_mae: 10.9500 - val_mse: 180.0170 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 542ms/step - loss: 6.5045 - mae: 6.5045 - mse: 63.3247 - val_loss: 9.4464 - val_mae: 9.4464 - val_mse: 158.6816 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 546ms/step - loss: 6.7588 - mae: 6.7588 - mse: 63.9908 - val_loss: 9.4073 - val_mae: 9.4073 - val_mse: 167.1731 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 568ms/step - loss: 7.1133 - mae: 7.1133 - mse: 76.6985 - val_loss: 9.3208 - val_mae: 9.3208 - val_mse: 155.2565 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 560ms/step - loss: 5.0255 - mae: 5.0255 - mse: 38.7138 - val_loss: 9.6186 - val_mae: 9.6186 - val_mse: 179.1781 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523ms/step - loss: 14.2277 - mae: 14.2277 - mse: 270.0441 - val_loss: 8.7415 - val_mae: 8.7415 - val_mse: 129.9378 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 533ms/step - loss: 13.4857 - mae: 13.4857 - mse: 253.8367 - val_loss: 13.0737 - val_mae: 13.0737 - val_mse: 228.8908 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 541ms/step - loss: 10.0298 - mae: 10.0298 - mse: 142.7319 - val_loss: 13.7519 - val_mae: 13.7519 - val_mse: 251.2493 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 579ms/step - loss: 7.0826 - mae: 7.0826 - mse: 70.3587 - val_loss: 7.8348 - val_mae: 7.8348 - val_mse: 104.4268 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 594ms/step - loss: 6.1095 - mae: 6.1095 - mse: 53.4818 - val_loss: 7.2797 - val_mae: 7.2797 - val_mse: 118.9540 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544ms/step - loss: 4.5282 - mae: 4.5282 - mse: 31.4070 - val_loss: 8.5013 - val_mae: 8.5013 - val_mse: 153.9332 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 507ms/step - loss: 4.1057 - mae: 4.1057 - mse: 26.7020 - val_loss: 8.3009 - val_mae: 8.3009 - val_mse: 146.6020 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 494ms/step - loss: 7.6243 - mae: 7.6243 - mse: 96.5555 - val_loss: 10.4810 - val_mae: 10.4810 - val_mse: 174.2817 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 514ms/step - loss: 16.2535 - mae: 16.2535 - mse: 356.9301 - val_loss: 9.0905 - val_mae: 9.0905 - val_mse: 120.5775 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549ms/step - loss: 13.3566 - mae: 13.3566 - mse: 253.5278 - val_loss: 16.7258 - val_mae: 16.7258 - val_mse: 384.4849 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549ms/step - loss: 11.7369 - mae: 11.7369 - mse: 192.6478 - val_loss: 6.2187 - val_mae: 6.2187 - val_mse: 69.1808 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530ms/step - loss: 12.9705 - mae: 12.9705 - mse: 238.4496 - val_loss: 11.6437 - val_mae: 11.6437 - val_mse: 214.8948 - learning_rate: 0.0010\n",
      "Epoch 52/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 572ms/step - loss: 10.7141 - mae: 10.7141 - mse: 157.3864 - val_loss: 8.1892 - val_mae: 8.1892 - val_mse: 117.3577 - learning_rate: 0.0010\n",
      "Epoch 53/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 543ms/step - loss: 10.2174 - mae: 10.2174 - mse: 158.3440 - val_loss: 9.3965 - val_mae: 9.3965 - val_mse: 152.3224 - learning_rate: 0.0010\n",
      "Epoch 54/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 675ms/step - loss: 8.0000 - mae: 8.0000 - mse: 101.7716 - val_loss: 9.2210 - val_mae: 9.2210 - val_mse: 158.4243 - learning_rate: 0.0010\n",
      "Epoch 55/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 622ms/step - loss: 8.4151 - mae: 8.4151 - mse: 104.0787 - val_loss: 7.1578 - val_mae: 7.1578 - val_mse: 86.5894 - learning_rate: 0.0010\n",
      "Epoch 56/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 565ms/step - loss: 10.0870 - mae: 10.0870 - mse: 156.3989 - val_loss: 12.6463 - val_mae: 12.6463 - val_mse: 258.2782 - learning_rate: 0.0010\n",
      "Epoch 57/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 587ms/step - loss: 12.8087 - mae: 12.8087 - mse: 213.4148 - val_loss: 7.8164 - val_mae: 7.8164 - val_mse: 109.3024 - learning_rate: 0.0010\n",
      "Epoch 58/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 627ms/step - loss: 12.3457 - mae: 12.3457 - mse: 210.1587 - val_loss: 13.5516 - val_mae: 13.5516 - val_mse: 226.1505 - learning_rate: 0.0010\n",
      "Epoch 59/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 627ms/step - loss: 9.8542 - mae: 9.8542 - mse: 133.3070 - val_loss: 8.8976 - val_mae: 8.8976 - val_mse: 117.2746 - learning_rate: 0.0010\n",
      "Epoch 60/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 645ms/step - loss: 9.0173 - mae: 9.0173 - mse: 122.0650 - val_loss: 10.4879 - val_mae: 10.4879 - val_mse: 161.4204 - learning_rate: 0.0010\n",
      "Epoch 61/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 643ms/step - loss: 7.1685 - mae: 7.1685 - mse: 81.6540 - val_loss: 7.4467 - val_mae: 7.4467 - val_mse: 118.0656 - learning_rate: 0.0010\n",
      "Epoch 62/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 670ms/step - loss: 10.7201 - mae: 10.7201 - mse: 163.2760 - val_loss: 8.9924 - val_mae: 8.9924 - val_mse: 140.1100 - learning_rate: 0.0010\n",
      "Epoch 63/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 715ms/step - loss: 9.4334 - mae: 9.4334 - mse: 137.3972 - val_loss: 8.8181 - val_mae: 8.8181 - val_mse: 148.1868 - learning_rate: 0.0010\n",
      "Epoch 64/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 591ms/step - loss: 6.8678 - mae: 6.8678 - mse: 72.2480 - val_loss: 8.0813 - val_mae: 8.0813 - val_mse: 132.8180 - learning_rate: 0.0010\n",
      "Epoch 65/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 593ms/step - loss: 4.6170 - mae: 4.6170 - mse: 35.9263 - val_loss: 12.0484 - val_mae: 12.0484 - val_mse: 290.5490 - learning_rate: 0.0010\n",
      "Epoch 66/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 590ms/step - loss: 8.1533 - mae: 8.1533 - mse: 100.3725 - val_loss: 7.4876 - val_mae: 7.4876 - val_mse: 101.9072 - learning_rate: 0.0010\n",
      "Epoch 67/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 595ms/step - loss: 7.9370 - mae: 7.9370 - mse: 96.9894 - val_loss: 7.4874 - val_mae: 7.4874 - val_mse: 93.7110 - learning_rate: 0.0010\n",
      "Epoch 68/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 584ms/step - loss: 13.2015 - mae: 13.2015 - mse: 243.1092 - val_loss: 12.5090 - val_mae: 12.5090 - val_mse: 220.1023 - learning_rate: 0.0010\n",
      "Epoch 69/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 634ms/step - loss: 8.8018 - mae: 8.8018 - mse: 118.3609 - val_loss: 10.6544 - val_mae: 10.6544 - val_mse: 144.9159 - learning_rate: 0.0010\n",
      "Epoch 70/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 675ms/step - loss: 6.8936 - mae: 6.8936 - mse: 70.3857 - val_loss: 7.0724 - val_mae: 7.0724 - val_mse: 105.5541 - learning_rate: 0.0010\n",
      "Epoch 71/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 748ms/step - loss: 6.5033 - mae: 6.5033 - mse: 62.2528 - val_loss: 8.7794 - val_mae: 8.7794 - val_mse: 137.4698 - learning_rate: 0.0010\n",
      "Epoch 72/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 692ms/step - loss: 10.0856 - mae: 10.0856 - mse: 219.6695 - val_loss: 7.2164 - val_mae: 7.2164 - val_mse: 96.4425 - learning_rate: 0.0010\n",
      "Epoch 73/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 636ms/step - loss: 13.1064 - mae: 13.1064 - mse: 251.1350 - val_loss: 8.6186 - val_mae: 8.6186 - val_mse: 150.7130 - learning_rate: 0.0010\n",
      "Epoch 74/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 693ms/step - loss: 11.4086 - mae: 11.4086 - mse: 188.4001 - val_loss: 8.5243 - val_mae: 8.5243 - val_mse: 128.9436 - learning_rate: 0.0010\n",
      "Epoch 75/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 694ms/step - loss: 9.5218 - mae: 9.5218 - mse: 139.0238 - val_loss: 8.5927 - val_mae: 8.5927 - val_mse: 149.4942 - learning_rate: 0.0010\n",
      "Epoch 76/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 679ms/step - loss: 7.5186 - mae: 7.5186 - mse: 86.9036 - val_loss: 8.8768 - val_mae: 8.8768 - val_mse: 162.5442 - learning_rate: 0.0010\n",
      "Epoch 77/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 677ms/step - loss: 5.4738 - mae: 5.4738 - mse: 51.8573 - val_loss: 9.9160 - val_mae: 9.9160 - val_mse: 186.1158 - learning_rate: 0.0010\n",
      "Epoch 78/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 743ms/step - loss: 11.9575 - mae: 11.9575 - mse: 208.3018 - val_loss: 12.3229 - val_mae: 12.3229 - val_mse: 217.2093 - learning_rate: 0.0010\n",
      "Epoch 79/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 711ms/step - loss: 12.2813 - mae: 12.2813 - mse: 218.8212 - val_loss: 7.1188 - val_mae: 7.1188 - val_mse: 95.8638 - learning_rate: 0.0010\n",
      "Epoch 80/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 692ms/step - loss: 13.5165 - mae: 13.5165 - mse: 242.5536 - val_loss: 11.2354 - val_mae: 11.2354 - val_mse: 194.5277 - learning_rate: 0.0010\n",
      "Epoch 81/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 713ms/step - loss: 11.0371 - mae: 11.0371 - mse: 176.7670 - val_loss: 8.6212 - val_mae: 8.6212 - val_mse: 143.2316 - learning_rate: 0.0010\n",
      "Epoch 82/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 760ms/step - loss: 10.7351 - mae: 10.7351 - mse: 156.0811 - val_loss: 8.8676 - val_mae: 8.8676 - val_mse: 146.8052 - learning_rate: 0.0010\n",
      "Epoch 83/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 682ms/step - loss: 7.3582 - mae: 7.3582 - mse: 81.7302 - val_loss: 10.4322 - val_mae: 10.4322 - val_mse: 225.5616 - learning_rate: 0.0010\n",
      "Epoch 84/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 662ms/step - loss: 6.2722 - mae: 6.2722 - mse: 62.6953 - val_loss: 7.1506 - val_mae: 7.1506 - val_mse: 103.0634 - learning_rate: 0.0010\n",
      "Epoch 85/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 586ms/step - loss: 6.0883 - mae: 6.0883 - mse: 52.0704 - val_loss: 7.1097 - val_mae: 7.1097 - val_mse: 112.5873 - learning_rate: 0.0010\n",
      "Epoch 86/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 653ms/step - loss: 4.0432 - mae: 4.0432 - mse: 26.6437 - val_loss: 9.9717 - val_mae: 9.9717 - val_mse: 230.6600 - learning_rate: 0.0010\n",
      "Epoch 87/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 607ms/step - loss: 8.2023 - mae: 8.2023 - mse: 92.8463 - val_loss: 6.1516 - val_mae: 6.1516 - val_mse: 59.6841 - learning_rate: 0.0010\n",
      "Epoch 88/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 601ms/step - loss: 13.7412 - mae: 13.7412 - mse: 265.7137 - val_loss: 14.2579 - val_mae: 14.2579 - val_mse: 280.5205 - learning_rate: 0.0010\n",
      "Epoch 89/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 606ms/step - loss: 10.2424 - mae: 10.2424 - mse: 146.1589 - val_loss: 9.2159 - val_mae: 9.2159 - val_mse: 123.0369 - learning_rate: 0.0010\n",
      "Epoch 90/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 661ms/step - loss: 10.5750 - mae: 10.5750 - mse: 155.4501 - val_loss: 7.2925 - val_mae: 7.2925 - val_mse: 87.9368 - learning_rate: 0.0010\n",
      "Epoch 91/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 659ms/step - loss: 8.2765 - mae: 8.2765 - mse: 99.2276 - val_loss: 8.2026 - val_mae: 8.2026 - val_mse: 129.5437 - learning_rate: 0.0010\n",
      "Epoch 92/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738ms/step - loss: 9.4971 - mae: 9.4971 - mse: 124.4523 - val_loss: 8.5107 - val_mae: 8.5107 - val_mse: 134.7940 - learning_rate: 0.0010\n",
      "Epoch 93/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 708ms/step - loss: 6.8253 - mae: 6.8253 - mse: 68.8220 - val_loss: 8.9216 - val_mae: 8.9216 - val_mse: 156.1253 - learning_rate: 0.0010\n",
      "Epoch 94/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 686ms/step - loss: 4.3543 - mae: 4.3543 - mse: 29.4070 - val_loss: 7.7005 - val_mae: 7.7005 - val_mse: 129.4969 - learning_rate: 0.0010\n",
      "Epoch 95/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 679ms/step - loss: 8.6024 - mae: 8.6024 - mse: 115.0352 - val_loss: 11.3550 - val_mae: 11.3550 - val_mse: 186.3090 - learning_rate: 0.0010\n",
      "Epoch 96/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 744ms/step - loss: 4.8784 - mae: 4.8784 - mse: 40.6556 - val_loss: 9.0762 - val_mae: 9.0762 - val_mse: 173.6354 - learning_rate: 0.0010\n",
      "Epoch 97/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 839ms/step - loss: 12.5997 - mae: 12.5997 - mse: 215.5081 - val_loss: 7.1043 - val_mae: 7.1043 - val_mse: 86.1410 - learning_rate: 0.0010\n",
      "Epoch 98/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 731ms/step - loss: 12.1014 - mae: 12.1014 - mse: 212.8782 - val_loss: 13.2933 - val_mae: 13.2933 - val_mse: 231.8167 - learning_rate: 0.0010\n",
      "Epoch 99/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 787ms/step - loss: 7.7561 - mae: 7.7561 - mse: 90.9957 - val_loss: 8.0466 - val_mae: 8.0466 - val_mse: 120.7823 - learning_rate: 0.0010\n",
      "Epoch 100/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 566ms/step - loss: 9.4194 - mae: 9.4194 - mse: 121.2555 - val_loss: 9.0714 - val_mae: 9.0714 - val_mse: 133.7909 - learning_rate: 0.0010\n",
      "Epoch 101/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - loss: 7.3397 - mae: 7.3397 - mse: 75.1038 - val_loss: 8.2021 - val_mae: 8.2021 - val_mse: 132.5647 - learning_rate: 0.0010\n",
      "Epoch 102/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 569ms/step - loss: 4.8101 - mae: 4.8101 - mse: 37.1074 - val_loss: 8.3913 - val_mae: 8.3913 - val_mse: 160.8503 - learning_rate: 0.0010\n",
      "Epoch 103/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 580ms/step - loss: 4.7774 - mae: 4.7774 - mse: 33.6079 - val_loss: 7.4266 - val_mae: 7.4266 - val_mse: 123.0335 - learning_rate: 0.0010\n",
      "Epoch 104/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 546ms/step - loss: 4.3422 - mae: 4.3422 - mse: 30.3061 - val_loss: 8.3975 - val_mae: 8.3975 - val_mse: 164.9623 - learning_rate: 0.0010\n",
      "Epoch 105/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 592ms/step - loss: 5.6216 - mae: 5.6216 - mse: 44.9769 - val_loss: 7.8906 - val_mae: 7.8906 - val_mse: 113.9039 - learning_rate: 0.0010\n",
      "Epoch 106/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 555ms/step - loss: 8.5284 - mae: 8.5284 - mse: 102.0325 - val_loss: 10.8586 - val_mae: 10.8586 - val_mse: 148.7722 - learning_rate: 0.0010\n",
      "Epoch 107/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 632ms/step - loss: 4.7115 - mae: 4.7115 - mse: 38.3967 - val_loss: 8.7827 - val_mae: 8.7827 - val_mse: 159.4130 - learning_rate: 0.0010\n",
      "Epoch 108/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 595ms/step - loss: 8.5516 - mae: 8.5516 - mse: 100.4510 - val_loss: 8.4800 - val_mae: 8.4800 - val_mse: 158.5693 - learning_rate: 0.0010\n",
      "Epoch 109/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - loss: 7.2958 - mae: 7.2958 - mse: 76.2445 - val_loss: 7.1371 - val_mae: 7.1371 - val_mse: 107.9397 - learning_rate: 0.0010\n",
      "Epoch 110/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 566ms/step - loss: 4.4646 - mae: 4.4646 - mse: 30.8122 - val_loss: 8.3787 - val_mae: 8.3787 - val_mse: 154.9474 - learning_rate: 0.0010\n",
      "Epoch 111/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561ms/step - loss: 3.2194 - mae: 3.2194 - mse: 17.0055 - val_loss: 8.6607 - val_mae: 8.6607 - val_mse: 146.7420 - learning_rate: 0.0010\n",
      "Epoch 112/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 545ms/step - loss: 4.6200 - mae: 4.6200 - mse: 32.7343 - val_loss: 7.2462 - val_mae: 7.2462 - val_mse: 122.3226 - learning_rate: 0.0010\n",
      "Epoch 113/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 563ms/step - loss: 5.5412 - mae: 5.5412 - mse: 47.5158 - val_loss: 9.6788 - val_mae: 9.6788 - val_mse: 200.4724 - learning_rate: 0.0010\n",
      "Epoch 114/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 616ms/step - loss: 11.3312 - mae: 11.3312 - mse: 170.7319 - val_loss: 7.6229 - val_mae: 7.6229 - val_mse: 120.2511 - learning_rate: 0.0010\n",
      "Epoch 115/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 563ms/step - loss: 10.0049 - mae: 10.0049 - mse: 143.9625 - val_loss: 11.7453 - val_mae: 11.7453 - val_mse: 169.5632 - learning_rate: 0.0010\n",
      "Epoch 116/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561ms/step - loss: 5.7274 - mae: 5.7274 - mse: 58.5636 - val_loss: 7.4968 - val_mae: 7.4968 - val_mse: 121.8905 - learning_rate: 0.0010\n",
      "Epoch 117/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544ms/step - loss: 7.0450 - mae: 7.0450 - mse: 65.9504 - val_loss: 10.3718 - val_mae: 10.3718 - val_mse: 176.2311 - learning_rate: 0.0010\n",
      "Epoch 118/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 567ms/step - loss: 4.5755 - mae: 4.5755 - mse: 31.9333 - val_loss: 8.0255 - val_mae: 8.0255 - val_mse: 142.4615 - learning_rate: 0.0010\n",
      "Epoch 119/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 673ms/step - loss: 3.7943 - mae: 3.7943 - mse: 23.4383 - val_loss: 7.9112 - val_mae: 7.9112 - val_mse: 142.5723 - learning_rate: 0.0010\n",
      "Epoch 120/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 565ms/step - loss: 4.2868 - mae: 4.2868 - mse: 32.5025 - val_loss: 10.0925 - val_mae: 10.0925 - val_mse: 222.1217 - learning_rate: 0.0010\n",
      "Epoch 121/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 570ms/step - loss: 12.6477 - mae: 12.6477 - mse: 212.1272 - val_loss: 8.2113 - val_mae: 8.2113 - val_mse: 122.6822 - learning_rate: 0.0010\n",
      "Epoch 122/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 640ms/step - loss: 11.2535 - mae: 11.2535 - mse: 182.8787 - val_loss: 7.3303 - val_mae: 7.3303 - val_mse: 99.1756 - learning_rate: 0.0010\n",
      "Epoch 123/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 647ms/step - loss: 5.9812 - mae: 5.9812 - mse: 51.6922 - val_loss: 12.0251 - val_mae: 12.0251 - val_mse: 204.9439 - learning_rate: 0.0010\n",
      "Epoch 124/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 648ms/step - loss: 4.1474 - mae: 4.1474 - mse: 30.1526 - val_loss: 8.1034 - val_mae: 8.1034 - val_mse: 135.1233 - learning_rate: 0.0010\n",
      "Epoch 125/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 632ms/step - loss: 6.7054 - mae: 6.7054 - mse: 66.3955 - val_loss: 8.0256 - val_mae: 8.0256 - val_mse: 129.4173 - learning_rate: 0.0010\n",
      "Epoch 126/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 640ms/step - loss: 4.2662 - mae: 4.2662 - mse: 27.3343 - val_loss: 7.8208 - val_mae: 7.8208 - val_mse: 136.3089 - learning_rate: 0.0010\n",
      "Epoch 127/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586ms/step - loss: 2.6879 - mae: 2.6879 - mse: 13.6822\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - loss: 2.7099 - mae: 2.7099 - mse: 13.8535 - val_loss: 8.8282 - val_mae: 8.8282 - val_mse: 169.0360 - learning_rate: 0.0010\n",
      "Epoch 128/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 687ms/step - loss: 2.5843 - mae: 2.5843 - mse: 10.5960 - val_loss: 8.6144 - val_mae: 8.6144 - val_mse: 163.3868 - learning_rate: 1.0000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 595ms/step - loss: 2.1332 - mae: 2.1332 - mse: 7.2714 - val_loss: 8.2735 - val_mae: 8.2735 - val_mse: 145.4731 - learning_rate: 1.0000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 596ms/step - loss: 1.8685 - mae: 1.8685 - mse: 5.6993 - val_loss: 8.1494 - val_mae: 8.1494 - val_mse: 137.6267 - learning_rate: 1.0000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 548ms/step - loss: 1.8316 - mae: 1.8316 - mse: 5.5764 - val_loss: 8.2501 - val_mae: 8.2501 - val_mse: 142.5692 - learning_rate: 1.0000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 587ms/step - loss: 1.7438 - mae: 1.7438 - mse: 5.0185 - val_loss: 8.3731 - val_mae: 8.3731 - val_mse: 150.3694 - learning_rate: 1.0000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 560ms/step - loss: 1.6850 - mae: 1.6850 - mse: 4.6346 - val_loss: 8.3594 - val_mae: 8.3594 - val_mse: 150.8798 - learning_rate: 1.0000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 538ms/step - loss: 1.6403 - mae: 1.6403 - mse: 4.4038 - val_loss: 8.2478 - val_mae: 8.2478 - val_mse: 146.1855 - learning_rate: 1.0000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 540ms/step - loss: 1.5824 - mae: 1.5824 - mse: 4.1152 - val_loss: 8.1552 - val_mae: 8.1552 - val_mse: 143.7607 - learning_rate: 1.0000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 587ms/step - loss: 1.5456 - mae: 1.5456 - mse: 3.9154 - val_loss: 8.1646 - val_mae: 8.1646 - val_mse: 145.2035 - learning_rate: 1.0000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 534ms/step - loss: 1.5131 - mae: 1.5131 - mse: 3.7428 - val_loss: 8.2214 - val_mae: 8.2214 - val_mse: 146.8554 - learning_rate: 1.0000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550ms/step - loss: 1.4811 - mae: 1.4811 - mse: 3.6033 - val_loss: 8.2297 - val_mae: 8.2297 - val_mse: 146.8542 - learning_rate: 1.0000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 566ms/step - loss: 1.4546 - mae: 1.4546 - mse: 3.4866 - val_loss: 8.1910 - val_mae: 8.1910 - val_mse: 145.9112 - learning_rate: 1.0000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552ms/step - loss: 1.4273 - mae: 1.4273 - mse: 3.3534 - val_loss: 8.1688 - val_mae: 8.1688 - val_mse: 145.1472 - learning_rate: 1.0000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530ms/step - loss: 1.4042 - mae: 1.4042 - mse: 3.2499 - val_loss: 8.1608 - val_mae: 8.1608 - val_mse: 145.2746 - learning_rate: 1.0000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 503ms/step - loss: 1.3804 - mae: 1.3804 - mse: 3.1511 - val_loss: 8.1485 - val_mae: 8.1485 - val_mse: 145.0947 - learning_rate: 1.0000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 521ms/step - loss: 1.3572 - mae: 1.3572 - mse: 3.0554 - val_loss: 8.1242 - val_mae: 8.1242 - val_mse: 144.6822 - learning_rate: 1.0000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 522ms/step - loss: 1.3360 - mae: 1.3360 - mse: 2.9646 - val_loss: 8.1211 - val_mae: 8.1211 - val_mse: 144.6196 - learning_rate: 1.0000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 527ms/step - loss: 1.3162 - mae: 1.3162 - mse: 2.8854 - val_loss: 8.1222 - val_mae: 8.1222 - val_mse: 144.7795 - learning_rate: 1.0000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528ms/step - loss: 1.2978 - mae: 1.2978 - mse: 2.8106 - val_loss: 8.1077 - val_mae: 8.1077 - val_mse: 144.3923 - learning_rate: 1.0000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 527ms/step - loss: 1.2799 - mae: 1.2799 - mse: 2.7384 - val_loss: 8.1015 - val_mae: 8.1015 - val_mse: 144.2255 - learning_rate: 1.0000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 533ms/step - loss: 1.2635 - mae: 1.2635 - mse: 2.6752 - val_loss: 8.1069 - val_mae: 8.1069 - val_mse: 144.4059 - learning_rate: 1.0000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 543ms/step - loss: 1.2476 - mae: 1.2476 - mse: 2.6188 - val_loss: 8.0998 - val_mae: 8.0998 - val_mse: 144.6347 - learning_rate: 1.0000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552ms/step - loss: 1.2306 - mae: 1.2306 - mse: 2.5479 - val_loss: 8.0822 - val_mae: 8.0822 - val_mse: 143.7753 - learning_rate: 1.0000e-04\n",
      "Epoch 151/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544ms/step - loss: 1.2149 - mae: 1.2149 - mse: 2.4935 - val_loss: 8.0764 - val_mae: 8.0764 - val_mse: 143.9779 - learning_rate: 1.0000e-04\n",
      "Epoch 152/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 531ms/step - loss: 1.1997 - mae: 1.1997 - mse: 2.4393 - val_loss: 8.0890 - val_mae: 8.0890 - val_mse: 144.0915 - learning_rate: 1.0000e-04\n",
      "Epoch 153/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519ms/step - loss: 1.1863 - mae: 1.1863 - mse: 2.3940 - val_loss: 8.0723 - val_mae: 8.0723 - val_mse: 143.8225 - learning_rate: 1.0000e-04\n",
      "Epoch 154/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535ms/step - loss: 1.1715 - mae: 1.1715 - mse: 2.3347 - val_loss: 8.0755 - val_mae: 8.0755 - val_mse: 143.6225 - learning_rate: 1.0000e-04\n",
      "Epoch 155/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 505ms/step - loss: 1.1587 - mae: 1.1587 - mse: 2.2915 - val_loss: 8.0780 - val_mae: 8.0780 - val_mse: 144.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 156/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530ms/step - loss: 1.1446 - mae: 1.1446 - mse: 2.2389 - val_loss: 8.0648 - val_mae: 8.0648 - val_mse: 143.3336 - learning_rate: 1.0000e-04\n",
      "Epoch 157/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 506ms/step - loss: 1.1318 - mae: 1.1318 - mse: 2.1999 - val_loss: 8.0578 - val_mae: 8.0578 - val_mse: 143.9841 - learning_rate: 1.0000e-04\n",
      "Epoch 158/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 511ms/step - loss: 1.1175 - mae: 1.1175 - mse: 2.1465 - val_loss: 8.0726 - val_mae: 8.0726 - val_mse: 143.8067 - learning_rate: 1.0000e-04\n",
      "Epoch 159/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 521ms/step - loss: 1.1049 - mae: 1.1049 - mse: 2.1146 - val_loss: 8.0628 - val_mae: 8.0628 - val_mse: 144.3726 - learning_rate: 1.0000e-04\n",
      "Epoch 160/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 534ms/step - loss: 1.0887 - mae: 1.0887 - mse: 2.0562 - val_loss: 8.0496 - val_mae: 8.0496 - val_mse: 143.2582 - learning_rate: 1.0000e-04\n",
      "Epoch 161/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 513ms/step - loss: 1.0773 - mae: 1.0773 - mse: 2.0186 - val_loss: 8.0453 - val_mae: 8.0453 - val_mse: 143.2907 - learning_rate: 1.0000e-04\n",
      "Epoch 162/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524ms/step - loss: 1.0673 - mae: 1.0673 - mse: 1.9816 - val_loss: 8.0562 - val_mae: 8.0562 - val_mse: 143.5580 - learning_rate: 1.0000e-04\n",
      "Epoch 163/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 566ms/step - loss: 1.0572 - mae: 1.0572 - mse: 1.9579 - val_loss: 8.0501 - val_mae: 8.0501 - val_mse: 143.7150 - learning_rate: 1.0000e-04\n",
      "Epoch 164/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532ms/step - loss: 1.0488 - mae: 1.0488 - mse: 1.9135 - val_loss: 8.0393 - val_mae: 8.0393 - val_mse: 142.9458 - learning_rate: 1.0000e-04\n",
      "Epoch 165/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 565ms/step - loss: 1.0420 - mae: 1.0420 - mse: 1.8960 - val_loss: 8.0711 - val_mae: 8.0711 - val_mse: 144.6457 - learning_rate: 1.0000e-04\n",
      "Epoch 166/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 593ms/step - loss: 1.0269 - mae: 1.0269 - mse: 1.8404 - val_loss: 8.0286 - val_mae: 8.0286 - val_mse: 142.4886 - learning_rate: 1.0000e-04\n",
      "Epoch 167/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 575ms/step - loss: 1.0132 - mae: 1.0132 - mse: 1.8081 - val_loss: 8.0469 - val_mae: 8.0469 - val_mse: 144.0542 - learning_rate: 1.0000e-04\n",
      "Epoch 168/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 531ms/step - loss: 0.9991 - mae: 0.9991 - mse: 1.7610 - val_loss: 8.0441 - val_mae: 8.0441 - val_mse: 143.0791 - learning_rate: 1.0000e-04\n",
      "Epoch 169/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 533ms/step - loss: 0.9877 - mae: 0.9877 - mse: 1.7333 - val_loss: 8.0361 - val_mae: 8.0361 - val_mse: 143.7768 - learning_rate: 1.0000e-04\n",
      "Epoch 170/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 512ms/step - loss: 0.9783 - mae: 0.9783 - mse: 1.6953 - val_loss: 8.0394 - val_mae: 8.0394 - val_mse: 142.6798 - learning_rate: 1.0000e-04\n",
      "Epoch 171/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544ms/step - loss: 0.9714 - mae: 0.9714 - mse: 1.6809 - val_loss: 8.0339 - val_mae: 8.0339 - val_mse: 143.8268 - learning_rate: 1.0000e-04\n",
      "Epoch 172/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 506ms/step - loss: 0.9630 - mae: 0.9630 - mse: 1.6392 - val_loss: 8.0306 - val_mae: 8.0306 - val_mse: 142.0455 - learning_rate: 1.0000e-04\n",
      "Epoch 173/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 520ms/step - loss: 0.9483 - mae: 0.9483 - mse: 1.6097 - val_loss: 8.0434 - val_mae: 8.0434 - val_mse: 144.3899 - learning_rate: 1.0000e-04\n",
      "Epoch 174/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 583ms/step - loss: 0.9343 - mae: 0.9343 - mse: 1.5608 - val_loss: 8.0045 - val_mae: 8.0045 - val_mse: 141.6060 - learning_rate: 1.0000e-04\n",
      "Epoch 175/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 522ms/step - loss: 0.9217 - mae: 0.9217 - mse: 1.5305 - val_loss: 8.0768 - val_mae: 8.0768 - val_mse: 145.1057 - learning_rate: 1.0000e-04\n",
      "Epoch 176/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549ms/step - loss: 0.9116 - mae: 0.9116 - mse: 1.5018 - val_loss: 8.0198 - val_mae: 8.0198 - val_mse: 142.4032 - learning_rate: 1.0000e-04\n",
      "Epoch 177/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 541ms/step - loss: 0.9028 - mae: 0.9028 - mse: 1.4631 - val_loss: 8.0699 - val_mae: 8.0699 - val_mse: 144.1620 - learning_rate: 1.0000e-04\n",
      "Epoch 178/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 506ms/step - loss: 0.8861 - mae: 0.8861 - mse: 1.4399 - val_loss: 8.0190 - val_mae: 8.0190 - val_mse: 142.8028 - learning_rate: 1.0000e-04\n",
      "Epoch 179/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 509ms/step - loss: 0.8703 - mae: 0.8703 - mse: 1.3766 - val_loss: 8.0595 - val_mae: 8.0595 - val_mse: 143.6981 - learning_rate: 1.0000e-04\n",
      "Epoch 180/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 514ms/step - loss: 0.8601 - mae: 0.8601 - mse: 1.3674 - val_loss: 8.0257 - val_mae: 8.0257 - val_mse: 142.3909 - learning_rate: 1.0000e-04\n",
      "Epoch 181/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 494ms/step - loss: 0.8562 - mae: 0.8562 - mse: 1.3307 - val_loss: 8.0108 - val_mae: 8.0108 - val_mse: 142.9217 - learning_rate: 1.0000e-04\n",
      "Epoch 182/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 590ms/step - loss: 0.8654 - mae: 0.8654 - mse: 1.3505 - val_loss: 8.0294 - val_mae: 8.0294 - val_mse: 142.2509 - learning_rate: 1.0000e-04\n",
      "Epoch 183/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 552ms/step - loss: 0.8926 - mae: 0.8926 - mse: 1.3981 - val_loss: 7.9407 - val_mae: 7.9407 - val_mse: 140.4257 - learning_rate: 1.0000e-04\n",
      "Epoch 184/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 565ms/step - loss: 0.9015 - mae: 0.9015 - mse: 1.3986 - val_loss: 8.0628 - val_mae: 8.0628 - val_mse: 144.5811 - learning_rate: 1.0000e-04\n",
      "Epoch 185/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552ms/step - loss: 0.8786 - mae: 0.8786 - mse: 1.3693 - val_loss: 7.9504 - val_mae: 7.9504 - val_mse: 140.9250 - learning_rate: 1.0000e-04\n",
      "Epoch 186/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 494ms/step - loss: 0.8450 - mae: 0.8450 - mse: 1.2552 - val_loss: 8.0722 - val_mae: 8.0722 - val_mse: 144.2542 - learning_rate: 1.0000e-04\n",
      "Epoch 187/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 509ms/step - loss: 0.8218 - mae: 0.8218 - mse: 1.2261 - val_loss: 8.0005 - val_mae: 8.0005 - val_mse: 142.8774 - learning_rate: 1.0000e-04\n",
      "Epoch 188/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 497ms/step - loss: 0.8218 - mae: 0.8218 - mse: 1.1948 - val_loss: 8.0485 - val_mae: 8.0485 - val_mse: 142.8500 - learning_rate: 1.0000e-04\n",
      "Epoch 189/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 501ms/step - loss: 0.8155 - mae: 0.8155 - mse: 1.2022 - val_loss: 7.9484 - val_mae: 7.9484 - val_mse: 141.4367 - learning_rate: 1.0000e-04\n",
      "Epoch 190/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 496ms/step - loss: 0.8140 - mae: 0.8140 - mse: 1.1614 - val_loss: 8.0593 - val_mae: 8.0593 - val_mse: 142.7857 - learning_rate: 1.0000e-04\n",
      "Epoch 191/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 516ms/step - loss: 0.8358 - mae: 0.8358 - mse: 1.2444 - val_loss: 7.9432 - val_mae: 7.9432 - val_mse: 142.0087 - learning_rate: 1.0000e-04\n",
      "Epoch 192/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 499ms/step - loss: 0.8553 - mae: 0.8553 - mse: 1.2421 - val_loss: 8.0751 - val_mae: 8.0751 - val_mse: 143.2850 - learning_rate: 1.0000e-04\n",
      "Epoch 193/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 514ms/step - loss: 0.8274 - mae: 0.8274 - mse: 1.2192 - val_loss: 7.9292 - val_mae: 7.9292 - val_mse: 141.5730 - learning_rate: 1.0000e-04\n",
      "Epoch 194/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 507ms/step - loss: 0.8170 - mae: 0.8170 - mse: 1.1386 - val_loss: 8.0506 - val_mae: 8.0506 - val_mse: 142.6269 - learning_rate: 1.0000e-04\n",
      "Epoch 195/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561ms/step - loss: 0.8027 - mae: 0.8027 - mse: 1.1454 - val_loss: 7.9327 - val_mae: 7.9327 - val_mse: 141.5169 - learning_rate: 1.0000e-04\n",
      "Epoch 196/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 509ms/step - loss: 0.8076 - mae: 0.8076 - mse: 1.1070 - val_loss: 8.0244 - val_mae: 8.0244 - val_mse: 142.2355 - learning_rate: 1.0000e-04\n",
      "Epoch 197/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 505ms/step - loss: 0.8095 - mae: 0.8095 - mse: 1.1439 - val_loss: 7.9258 - val_mae: 7.9258 - val_mse: 140.5942 - learning_rate: 1.0000e-04\n",
      "Epoch 198/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496ms/step - loss: 0.7936 - mae: 0.7936 - mse: 1.0685 - val_loss: 8.0318 - val_mae: 8.0318 - val_mse: 143.3053 - learning_rate: 1.0000e-04\n",
      "Epoch 199/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530ms/step - loss: 0.7741 - mae: 0.7741 - mse: 1.0712 - val_loss: 7.9289 - val_mae: 7.9289 - val_mse: 140.2696 - learning_rate: 1.0000e-04\n",
      "Epoch 200/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 492ms/step - loss: 0.7481 - mae: 0.7481 - mse: 0.9700 - val_loss: 8.0294 - val_mae: 8.0294 - val_mse: 143.3398 - learning_rate: 1.0000e-04\n",
      "Epoch 201/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 508ms/step - loss: 0.7206 - mae: 0.7206 - mse: 0.9515 - val_loss: 7.9349 - val_mae: 7.9349 - val_mse: 140.9924 - learning_rate: 1.0000e-04\n",
      "Epoch 202/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 556ms/step - loss: 0.7226 - mae: 0.7226 - mse: 0.9110 - val_loss: 8.0097 - val_mae: 8.0097 - val_mse: 142.3048 - learning_rate: 1.0000e-04\n",
      "Epoch 203/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 511ms/step - loss: 0.7116 - mae: 0.7116 - mse: 0.9159 - val_loss: 7.9385 - val_mae: 7.9385 - val_mse: 141.0319 - learning_rate: 1.0000e-04\n",
      "Epoch 204/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 513ms/step - loss: 0.7020 - mae: 0.7020 - mse: 0.8630 - val_loss: 8.0001 - val_mae: 8.0001 - val_mse: 142.1091 - learning_rate: 1.0000e-04\n",
      "Epoch 205/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 495ms/step - loss: 0.7026 - mae: 0.7026 - mse: 0.8998 - val_loss: 7.9289 - val_mae: 7.9289 - val_mse: 140.8561 - learning_rate: 1.0000e-04\n",
      "Epoch 206/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524ms/step - loss: 0.7219 - mae: 0.7219 - mse: 0.8945 - val_loss: 8.0478 - val_mae: 8.0478 - val_mse: 143.2370 - learning_rate: 1.0000e-04\n",
      "Epoch 207/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 516ms/step - loss: 0.7357 - mae: 0.7357 - mse: 0.9704 - val_loss: 7.8659 - val_mae: 7.8659 - val_mse: 139.5467 - learning_rate: 1.0000e-04\n",
      "Epoch 208/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523ms/step - loss: 0.7743 - mae: 0.7743 - mse: 1.0080 - val_loss: 8.1026 - val_mae: 8.1026 - val_mse: 144.4722 - learning_rate: 1.0000e-04\n",
      "Epoch 209/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 502ms/step - loss: 0.7379 - mae: 0.7379 - mse: 0.9949 - val_loss: 7.9271 - val_mae: 7.9271 - val_mse: 140.8495 - learning_rate: 1.0000e-04\n",
      "Epoch 210/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519ms/step - loss: 0.7177 - mae: 0.7177 - mse: 0.8733 - val_loss: 8.1073 - val_mae: 8.1073 - val_mse: 144.5730 - learning_rate: 1.0000e-04\n",
      "Epoch 211/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 516ms/step - loss: 0.7302 - mae: 0.7302 - mse: 0.9385 - val_loss: 7.9209 - val_mae: 7.9209 - val_mse: 140.4778 - learning_rate: 1.0000e-04\n",
      "Epoch 212/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532ms/step - loss: 0.7455 - mae: 0.7455 - mse: 0.9118 - val_loss: 7.9889 - val_mae: 7.9889 - val_mse: 138.8290 - learning_rate: 1.0000e-04\n",
      "Epoch 213/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 533ms/step - loss: 0.6545 - mae: 0.6545 - mse: 0.7728 - val_loss: 7.9471 - val_mae: 7.9471 - val_mse: 144.1391 - learning_rate: 1.0000e-04\n",
      "Epoch 214/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552ms/step - loss: 0.6459 - mae: 0.6459 - mse: 0.7094 - val_loss: 8.0634 - val_mae: 8.0634 - val_mse: 141.7633 - learning_rate: 1.0000e-04\n",
      "Epoch 215/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 565ms/step - loss: 0.7037 - mae: 0.7037 - mse: 0.8789 - val_loss: 7.8576 - val_mae: 7.8576 - val_mse: 141.2698 - learning_rate: 1.0000e-04\n",
      "Epoch 216/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 541ms/step - loss: 0.8863 - mae: 0.8863 - mse: 1.1993 - val_loss: 8.1043 - val_mae: 8.1043 - val_mse: 141.2027 - learning_rate: 1.0000e-04\n",
      "Epoch 217/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 557ms/step - loss: 0.9084 - mae: 0.9084 - mse: 1.3194 - val_loss: 7.8599 - val_mae: 7.8599 - val_mse: 142.6806 - learning_rate: 1.0000e-04\n",
      "Epoch 218/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 548ms/step - loss: 0.8091 - mae: 0.8091 - mse: 1.0225 - val_loss: 8.0431 - val_mae: 8.0431 - val_mse: 140.0325 - learning_rate: 1.0000e-04\n",
      "Epoch 219/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535ms/step - loss: 0.7582 - mae: 0.7582 - mse: 0.9768 - val_loss: 7.8686 - val_mae: 7.8686 - val_mse: 142.5342 - learning_rate: 1.0000e-04\n",
      "Epoch 220/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 523ms/step - loss: 0.7926 - mae: 0.7926 - mse: 0.9892 - val_loss: 8.0809 - val_mae: 8.0809 - val_mse: 141.3808 - learning_rate: 1.0000e-04\n",
      "Epoch 221/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550ms/step - loss: 0.8088 - mae: 0.8088 - mse: 1.0913 - val_loss: 7.8622 - val_mae: 7.8622 - val_mse: 141.7893 - learning_rate: 1.0000e-04\n",
      "Epoch 222/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 505ms/step - loss: 0.7906 - mae: 0.7906 - mse: 0.9891 - val_loss: 8.0521 - val_mae: 8.0521 - val_mse: 140.9801 - learning_rate: 1.0000e-04\n",
      "Epoch 223/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 511ms/step - loss: 0.7602 - mae: 0.7602 - mse: 0.9810 - val_loss: 7.8752 - val_mae: 7.8752 - val_mse: 142.1300 - learning_rate: 1.0000e-04\n",
      "Epoch 224/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 511ms/step - loss: 0.7412 - mae: 0.7412 - mse: 0.8866 - val_loss: 8.0331 - val_mae: 8.0331 - val_mse: 140.5437 - learning_rate: 1.0000e-04\n",
      "Epoch 225/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 503ms/step - loss: 0.7071 - mae: 0.7071 - mse: 0.8690 - val_loss: 7.8922 - val_mae: 7.8922 - val_mse: 142.4256 - learning_rate: 1.0000e-04\n",
      "Epoch 226/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524ms/step - loss: 0.7090 - mae: 0.7090 - mse: 0.8101 - val_loss: 8.0254 - val_mae: 8.0254 - val_mse: 140.9082 - learning_rate: 1.0000e-04\n",
      "Epoch 227/5000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - loss: 0.6846 - mae: 0.6846 - mse: 0.8231\n",
      "Epoch 227: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 512ms/step - loss: 0.6827 - mae: 0.6827 - mse: 0.8178 - val_loss: 7.8628 - val_mae: 7.8628 - val_mse: 142.2614 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x73ee17abff50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Setup N-BEATS Block layer\n",
    "nbeats_block_layer = NBeatsBlock(input_size=60,\n",
    "                                 theta_size=THETA_SIZE,\n",
    "                                 horizon=HORIZON,\n",
    "                                 n_neurons=N_NEURONS,\n",
    "                                 n_layers=N_LAYERS,\n",
    "                                 name=\"InitialBlock\")\n",
    "\n",
    "# 2. Create input to stacks\n",
    "stack_input = layers.Input(shape=[60], name=\"stack_input\")\n",
    "\n",
    "# 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)\n",
    "backcast, forecast = nbeats_block_layer(stack_input)\n",
    "residuals = layers.subtract([stack_input, backcast], name=f\"subtract_00\") \n",
    "\n",
    "# 4. Create stacks of blocks\n",
    "for i, _ in enumerate(range(N_STACKS-1)): # first stack is already creted in (3)\n",
    "\n",
    "    # 5. Use the NBeatsBlock to calculate the backcast as well as block forecast\n",
    "    backcast, block_forecast = NBeatsBlock(\n",
    "        input_size=60,\n",
    "        theta_size=THETA_SIZE,\n",
    "        horizon=HORIZON,\n",
    "        n_neurons=N_NEURONS,\n",
    "        n_layers=N_LAYERS,\n",
    "        name=f\"NBeatsBlock_{i}\"\n",
    "    )(residuals) # pass it in residuals (the backcast)\n",
    "\n",
    "    # 6. Create the double residual stacking\n",
    "    residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\") \n",
    "    forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n",
    "\n",
    "# 7. Put the stack model together\n",
    "model_1 = keras.Model(inputs=stack_input, \n",
    "                         outputs=forecast, \n",
    "                         name=\"model_1_N-BEATS\")\n",
    "\n",
    "model_1.compile(loss=\"mae\",\n",
    "                optimizer=keras.optimizers.Adam(0.001),\n",
    "                metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "# 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
    "model_1.fit(train_dataset,\n",
    "            epochs=N_EPOCHS,\n",
    "            validation_data=test_dataset,\n",
    "            verbose=1, # prevent large amounts of training outputs\n",
    "            # callbacks=[create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time\n",
    "            callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, restore_best_weights=True),\n",
    "                      keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 5.7706 - mae: 5.7706 - mse: 67.4544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.047199726104736, 6.047199726104736, 73.7038803100586]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with N-BEATS model\n",
    "model_1_preds = model_1.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': <tf.Tensor: shape=(), dtype=float32, numpy=6.0471997>,\n",
       " 'mse': <tf.Tensor: shape=(), dtype=float32, numpy=73.70388>,\n",
       " 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=7.2709985>,\n",
       " 'mape': <tf.Tensor: shape=(), dtype=float32, numpy=7.671045>,\n",
       " 'mase': <tf.Tensor: shape=(), dtype=float32, numpy=6.6469483>}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluate N-BEATS model predictions\n",
    "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_models(horizon=HORIZON, \n",
    "                        train_data=train_dataset,\n",
    "                        test_data=test_dataset,\n",
    "                        num_iter=10, \n",
    "                        num_epochs=100, \n",
    "                        loss_fns=[\"mae\", \"mse\", \"mape\"]):\n",
    "    \"\"\"\n",
    "    Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.\n",
    "\n",
    "    For example, if num_iter=10, a list of 30 trained models will be returned:\n",
    "    10 * len([\"mae\", \"mse\", \"mape\"]).\n",
    "    \"\"\"\n",
    "    # Make empty list for trained ensemble models\n",
    "    ensemble_models = []\n",
    "\n",
    "    # Create num_iter number of models per loss function\n",
    "    for i in range(num_iter):\n",
    "        # Build and fit a new model with a different loss function\n",
    "        for loss_function in loss_fns:\n",
    "            print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")\n",
    "\n",
    "            # Construct a simple model (similar to model_1)\n",
    "            model = tf.keras.Sequential([\n",
    "                # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction\n",
    "                # interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal\n",
    "                layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"), \n",
    "                layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n",
    "                layers.Dense(HORIZON)                                 \n",
    "            ])\n",
    "\n",
    "            # Compile simple model with current loss function\n",
    "            model.compile(loss=loss_function,\n",
    "                            optimizer=tf.keras.optimizers.Adam(),\n",
    "                            metrics=[\"mae\", \"mse\"])\n",
    "            \n",
    "            # Fit model\n",
    "            model.fit(train_data,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=0,\n",
    "                        validation_data=test_data,\n",
    "                        # Add callbacks to prevent training from going/stalling for too long\n",
    "                        callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                                    patience=200,\n",
    "                                                                    restore_best_weights=True),\n",
    "                                keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                                        patience=100,\n",
    "                                                                        verbose=1)])\n",
    "            \n",
    "            # Append fitted model to list of ensemble models\n",
    "            ensemble_models.append(model)\n",
    "\n",
    "    return ensemble_models # return list of trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing model by reducing: mae for 1000 epochs, model number: 0\n",
      "\n",
      "Epoch 226: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 326: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mse for 1000 epochs, model number: 0\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 276: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mape for 1000 epochs, model number: 0\n",
      "\n",
      "Epoch 258: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 358: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mae for 1000 epochs, model number: 1\n",
      "\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 307: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mse for 1000 epochs, model number: 1\n",
      "\n",
      "Epoch 213: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 313: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mape for 1000 epochs, model number: 1\n",
      "\n",
      "Epoch 445: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 545: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mae for 1000 epochs, model number: 2\n",
      "\n",
      "Epoch 322: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 422: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mse for 1000 epochs, model number: 2\n",
      "\n",
      "Epoch 302: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 402: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mape for 1000 epochs, model number: 2\n",
      "\n",
      "Epoch 255: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 355: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mae for 1000 epochs, model number: 3\n",
      "\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 320: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mse for 1000 epochs, model number: 3\n",
      "\n",
      "Epoch 367: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 467: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mape for 1000 epochs, model number: 3\n",
      "\n",
      "Epoch 313: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 413: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mae for 1000 epochs, model number: 4\n",
      "\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 340: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mse for 1000 epochs, model number: 4\n",
      "\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 351: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Optimizing model by reducing: mape for 1000 epochs, model number: 4\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 294: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n"
     ]
    }
   ],
   "source": [
    "ensemble_models = get_ensemble_models(num_iter=5,\n",
    "                                      num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble_preds(ensemble_models, data):\n",
    "    ensemble_preds = []\n",
    "    for model in ensemble_models:\n",
    "        preds = model.predict(data) # make predictions with current ensemble model\n",
    "        ensemble_preds.append(preds)\n",
    "    return tf.constant(tf.squeeze(ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15, 35, 30), dtype=float32, numpy=\n",
       "array([[[ 91.90076 ,  91.455284,  90.06599 , ...,  89.6185  ,\n",
       "          83.25336 ,  82.04163 ],\n",
       "        [ 94.000114,  92.84287 ,  87.92823 , ...,  87.28374 ,\n",
       "          83.414246,  81.08657 ],\n",
       "        [ 93.92259 ,  91.65942 ,  85.81731 , ...,  86.61658 ,\n",
       "          82.65299 ,  80.29374 ],\n",
       "        ...,\n",
       "        [ 87.50764 ,  86.01962 ,  89.39275 , ..., 105.26368 ,\n",
       "          89.239914,  94.85974 ],\n",
       "        [ 87.32938 ,  85.21523 ,  89.686   , ..., 107.876205,\n",
       "          89.74566 ,  96.59209 ],\n",
       "        [ 87.16394 ,  85.161224,  90.284386, ..., 108.92953 ,\n",
       "          91.231186,  98.93103 ]],\n",
       "\n",
       "       [[ 95.24617 ,  92.195946,  90.41885 , ...,  80.643555,\n",
       "          84.19835 ,  81.746605],\n",
       "        [ 94.94705 ,  89.035736,  89.57933 , ...,  79.265945,\n",
       "          83.73689 ,  81.55528 ],\n",
       "        [ 94.32059 ,  87.616135,  87.72722 , ...,  78.72259 ,\n",
       "          83.523506,  81.402435],\n",
       "        ...,\n",
       "        [ 94.75214 ,  87.993065,  85.203674, ..., 101.083595,\n",
       "         102.72498 , 101.69615 ],\n",
       "        [ 95.96571 ,  88.21482 ,  87.67812 , ..., 101.52807 ,\n",
       "         103.25644 , 103.06748 ],\n",
       "        [ 95.736336,  87.3819  ,  91.53448 , ..., 102.52485 ,\n",
       "         103.4439  , 104.16573 ]],\n",
       "\n",
       "       [[ 95.94054 ,  88.93293 ,  89.58299 , ...,  78.94815 ,\n",
       "          81.99022 ,  81.093994],\n",
       "        [ 96.36342 ,  89.036385,  87.694466, ...,  78.32659 ,\n",
       "          80.78731 ,  80.59301 ],\n",
       "        [ 92.67708 ,  86.50724 ,  87.223206, ...,  77.91574 ,\n",
       "          81.66344 ,  79.8011  ],\n",
       "        ...,\n",
       "        [ 89.54884 ,  85.74336 ,  82.81667 , ...,  97.19453 ,\n",
       "          98.67513 ,  98.09723 ],\n",
       "        [ 89.75078 ,  86.3461  ,  83.623055, ...,  97.62257 ,\n",
       "          99.38201 ,  98.278564],\n",
       "        [ 89.47353 ,  85.70059 ,  84.1937  , ...,  98.242195,\n",
       "          99.21375 ,  98.770775]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 92.72889 ,  93.70454 ,  90.90032 , ...,  87.61064 ,\n",
       "          86.74674 ,  86.9971  ],\n",
       "        [ 91.816666,  91.997765,  92.0461  , ...,  85.921074,\n",
       "          84.010284,  85.19587 ],\n",
       "        [ 92.37882 ,  90.73124 ,  92.36116 , ...,  85.28212 ,\n",
       "          81.841896,  84.66598 ],\n",
       "        ...,\n",
       "        [ 82.942955,  89.391464,  88.77761 , ..., 102.73619 ,\n",
       "         103.89611 , 104.35007 ],\n",
       "        [ 82.748276,  89.6841  ,  88.54413 , ..., 103.65194 ,\n",
       "         105.678474, 104.594025],\n",
       "        [ 83.528725,  90.07232 ,  87.90384 , ..., 103.86319 ,\n",
       "         107.62634 , 104.66728 ]],\n",
       "\n",
       "       [[ 90.21074 ,  90.68183 ,  90.29953 , ...,  78.43396 ,\n",
       "          89.89088 ,  84.15332 ],\n",
       "        [ 90.61317 ,  91.06303 ,  90.489815, ...,  78.6403  ,\n",
       "          91.77713 ,  84.01529 ],\n",
       "        [ 87.7087  ,  90.688965,  89.875084, ...,  79.35549 ,\n",
       "          92.08241 ,  82.7155  ],\n",
       "        ...,\n",
       "        [ 87.033585,  87.19803 ,  86.47165 , ...,  99.76698 ,\n",
       "          99.959305, 104.60161 ],\n",
       "        [ 87.43668 ,  86.99433 ,  85.88011 , ...,  99.09059 ,\n",
       "          99.915665, 105.37494 ],\n",
       "        [ 89.20801 ,  86.23397 ,  86.56274 , ..., 100.4628  ,\n",
       "         100.561874, 108.58013 ]],\n",
       "\n",
       "       [[ 93.96784 ,  92.33599 ,  86.245224, ...,  84.95133 ,\n",
       "          81.69689 ,  87.49293 ],\n",
       "        [ 94.23464 ,  90.79232 ,  85.270424, ...,  87.05258 ,\n",
       "          81.68281 ,  85.727234],\n",
       "        [ 94.90145 ,  90.56313 ,  83.83706 , ...,  87.93772 ,\n",
       "          81.8473  ,  86.164764],\n",
       "        ...,\n",
       "        [ 91.60727 ,  83.94476 ,  87.78348 , ..., 103.10873 ,\n",
       "         101.83637 , 106.17171 ],\n",
       "        [ 91.28406 ,  85.28505 ,  89.213684, ..., 103.91488 ,\n",
       "         103.89887 , 104.632835],\n",
       "        [ 89.616   ,  86.116646,  88.97516 , ..., 101.65308 ,\n",
       "         103.21039 , 105.49839 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of ensemble predictions\n",
    "ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models,\n",
    "                                     data=test_dataset)\n",
    "ensemble_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': <tf.Tensor: shape=(), dtype=float32, numpy=6.5977607>,\n",
       " 'mse': <tf.Tensor: shape=(), dtype=float32, numpy=97.955444>,\n",
       " 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=7.7654915>,\n",
       " 'mape': <tf.Tensor: shape=(), dtype=float32, numpy=8.418934>,\n",
       " 'mase': <tf.Tensor: shape=(), dtype=float32, numpy=7.2521114>}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate ensemble model(s) predictions\n",
    "ensemble_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                  y_pred=np.median(ensemble_preds, axis=0)) # take the median across all ensemble predictions\n",
    "ensemble_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': <tf.Tensor: shape=(), dtype=float32, numpy=6.0471997>,\n",
       " 'mse': <tf.Tensor: shape=(), dtype=float32, numpy=73.70388>,\n",
       " 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=7.2709985>,\n",
       " 'mape': <tf.Tensor: shape=(), dtype=float32, numpy=7.671045>,\n",
       " 'mase': <tf.Tensor: shape=(), dtype=float32, numpy=6.6469483>}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': <tf.Tensor: shape=(), dtype=float32, numpy=11.429537>,\n",
       " 'mse': <tf.Tensor: shape=(), dtype=float32, numpy=159.11499>,\n",
       " 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=12.430287>,\n",
       " 'mape': <tf.Tensor: shape=(), dtype=float32, numpy=14.108284>,\n",
       " 'mase': <tf.Tensor: shape=(), dtype=float32, numpy=12.563094>}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save_weights('model_experiments/model_1_NBEATS.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup N-BEATS Block layer\n",
    "nbeats_block_layer = NBeatsBlock(input_size=60,\n",
    "                                 theta_size=THETA_SIZE,\n",
    "                                 horizon=HORIZON,\n",
    "                                 n_neurons=N_NEURONS,\n",
    "                                 n_layers=N_LAYERS,\n",
    "                                 name=\"InitialBlock\")\n",
    "\n",
    "# 2. Create input to stacks\n",
    "stack_input = layers.Input(shape=[60], name=\"stack_input\")\n",
    "\n",
    "# 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)\n",
    "backcast, forecast = nbeats_block_layer(stack_input)\n",
    "residuals = layers.subtract([stack_input, backcast], name=f\"subtract_00\") \n",
    "\n",
    "# 4. Create stacks of blocks\n",
    "for i, _ in enumerate(range(N_STACKS-1)): # first stack is already creted in (3)\n",
    "\n",
    "    # 5. Use the NBeatsBlock to calculate the backcast as well as block forecast\n",
    "    backcast, block_forecast = NBeatsBlock(\n",
    "        input_size=60,\n",
    "        theta_size=THETA_SIZE,\n",
    "        horizon=HORIZON,\n",
    "        n_neurons=N_NEURONS,\n",
    "        n_layers=N_LAYERS,\n",
    "        name=f\"NBeatsBlock_{i}\"\n",
    "    )(residuals) # pass it in residuals (the backcast)\n",
    "\n",
    "    # 6. Create the double residual stacking\n",
    "    residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\") \n",
    "    forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n",
    "\n",
    "# 7. Put the stack model together\n",
    "loaded_model = keras.Model(inputs=stack_input, \n",
    "                         outputs=forecast, \n",
    "                         name=\"model_1_N-BEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mae': <tf.Tensor: shape=(), dtype=float32, numpy=76.3029>,\n",
       " 'mse': <tf.Tensor: shape=(), dtype=float32, numpy=7032.9097>,\n",
       " 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=83.81564>,\n",
       " 'mape': <tf.Tensor: shape=(), dtype=float32, numpy=92.027596>,\n",
       " 'mase': <tf.Tensor: shape=(), dtype=float32, numpy=83.87045>}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with N-BEATS model\n",
    "loaded_model_preds = loaded_model.predict(test_dataset)\n",
    "# Evaluate N-BEATS model predictions\n",
    "loaded_model_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=loaded_model_preds)\n",
    "loaded_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights('model_experiments/model_1_NBEATS.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mae': <tf.Tensor: shape=(), dtype=float32, numpy=6.0471997>,\n",
       " 'mse': <tf.Tensor: shape=(), dtype=float32, numpy=73.70388>,\n",
       " 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=7.2709985>,\n",
       " 'mape': <tf.Tensor: shape=(), dtype=float32, numpy=7.671045>,\n",
       " 'mase': <tf.Tensor: shape=(), dtype=float32, numpy=6.6469483>}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with N-BEATS model\n",
    "loaded_model_preds = loaded_model.predict(test_dataset)\n",
    "# Evaluate N-BEATS model predictions\n",
    "loaded_model_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=loaded_model_preds)\n",
    "loaded_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_results is model_1_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
